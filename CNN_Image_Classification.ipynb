{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVOLUTIONAL NEURAL NETWORK (CNN)\n",
    "## Image Classification with Deep Learning\n",
    "### CODTECH Internship Task\n",
    "\n",
    "**Objective:** Build a CNN model to classify images from the MNIST dataset\n",
    "\n",
    "**Dataset:** MNIST - Handwritten Digits (70,000 images, 28x28 pixels)\n",
    "\n",
    "**Framework:** TensorFlow/Keras\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LOAD DATASET\n",
    "\n",
    "**MNIST Dataset:**\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- 28x28 grayscale images\n",
    "- 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MNIST dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training images shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test images shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nImage dimensions: {X_train.shape[1]} x {X_train.shape[2]} pixels\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Distribution in Training Set:\")\n",
    "print(\"=\" * 60)\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for digit, count in zip(unique, counts):\n",
    "    print(f\"Digit {digit}: {count:5d} images ({count/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(unique, counts, color='#4ECDC4')\n",
    "axes[0].set_xlabel('Digit Class', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[0].set_title('Training Data Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "axes[1].bar(unique_test, counts_test, color='#FF6B6B')\n",
    "axes[1].set_xlabel('Digit Class', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[1].set_title('Test Data Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(counts_test):\n",
    "    axes[1].text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 10, figsize=(15, 8))\n",
    "fig.suptitle('Sample Images from MNIST Dataset', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for i in range(50):\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(X_train[i], cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPixel Value Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Minimum pixel value: {X_train.min()}\")\n",
    "print(f\"Maximum pixel value: {X_train.max()}\")\n",
    "print(f\"Mean pixel value: {X_train.mean():.2f}\")\n",
    "print(f\"Standard deviation: {X_train.std():.2f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(X_train.flatten(), bins=50, color='#4ECDC4', alpha=0.7)\n",
    "plt.xlabel('Pixel Value', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Pixel Value Distribution (Before Normalization)', fontsize=12, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "sample_img = X_train[0]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_img, cmap='hot')\n",
    "plt.colorbar(label='Pixel Intensity')\n",
    "plt.title(f'Sample Image Heatmap (Label: {y_train[0]})', fontsize=12, fontweight='bold')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DATA PREPROCESSING\n",
    "\n",
    "### Steps:\n",
    "1. **Reshape**: Add channel dimension for CNN (28, 28) â†’ (28, 28, 1)\n",
    "2. **Normalize**: Scale pixel values from [0, 255] â†’ [0, 1]\n",
    "3. **One-Hot Encode**: Convert labels to categorical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing Data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Reshaping images...\")\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "print(f\"   Training shape: {X_train_original.shape} â†’ {X_train.shape}\")\n",
    "print(f\"   Test shape: {X_test_original.shape} â†’ {X_test.shape}\")\n",
    "\n",
    "print(\"\\n2. Normalizing pixel values...\")\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "print(f\"   Pixel range before: [0, 255]\")\n",
    "print(f\"   Pixel range after: [{X_train.min():.1f}, {X_train.max():.1f}]\")\n",
    "\n",
    "print(\"\\n3. One-hot encoding labels...\")\n",
    "print(f\"   Label before: {y_train[0]} (integer)\")\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "print(f\"   Label after: {y_train_cat[0]} (one-hot vector)\")\n",
    "print(f\"   Training labels shape: {y_train.shape} â†’ {y_train_cat.shape}\")\n",
    "print(f\"   Test labels shape: {y_test.shape} â†’ {y_test_cat.shape}\")\n",
    "\n",
    "print(\"\\nâœ“ Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(X_train.flatten(), bins=50, color='#45B7D1', alpha=0.7)\n",
    "plt.xlabel('Normalized Pixel Value', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Pixel Value Distribution (After Normalization)', fontsize=12, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sample_labels = y_train_cat[:5]\n",
    "im = plt.imshow(sample_labels, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(im, label='Probability')\n",
    "plt.xlabel('Class (0-9)', fontsize=12)\n",
    "plt.ylabel('Sample Index', fontsize=12)\n",
    "plt.title('One-Hot Encoded Labels (First 5 samples)', fontsize=12, fontweight='bold')\n",
    "plt.xticks(range(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BUILD CNN ARCHITECTURE\n",
    "\n",
    "### Our CNN Architecture:\n",
    "\n",
    "```\n",
    "Input (28x28x1)\n",
    "    â†“\n",
    "Conv2D (32 filters, 3x3) + ReLU\n",
    "    â†“\n",
    "BatchNormalization\n",
    "    â†“\n",
    "Conv2D (32 filters, 3x3) + ReLU\n",
    "    â†“\n",
    "MaxPooling2D (2x2)\n",
    "    â†“\n",
    "Dropout (0.25)\n",
    "    â†“\n",
    "Conv2D (64 filters, 3x3) + ReLU\n",
    "    â†“\n",
    "BatchNormalization\n",
    "    â†“\n",
    "Conv2D (64 filters, 3x3) + ReLU\n",
    "    â†“\n",
    "MaxPooling2D (2x2)\n",
    "    â†“\n",
    "Dropout (0.25)\n",
    "    â†“\n",
    "Flatten\n",
    "    â†“\n",
    "Dense (256) + ReLU\n",
    "    â†“\n",
    "BatchNormalization\n",
    "    â†“\n",
    "Dropout (0.5)\n",
    "    â†“\n",
    "Dense (10) + Softmax\n",
    "    â†“\n",
    "Output (10 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building CNN Model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = Sequential([\n",
    "    # First Convolutional Block\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"\\nâœ“ Model architecture created!\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(\"=\" * 60)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "\n",
    "print(\"\\nModel Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Number of Layers: {len(model.layers)}\")\n",
    "print(f\"\\nConvolutional Layers: {sum(1 for layer in model.layers if isinstance(layer, Conv2D))}\")\n",
    "print(f\"Pooling Layers: {sum(1 for layer in model.layers if isinstance(layer, MaxPooling2D))}\")\n",
    "print(f\"Dense Layers: {sum(1 for layer in model.layers if isinstance(layer, Dense))}\")\n",
    "print(f\"Dropout Layers: {sum(1 for layer in model.layers if isinstance(layer, Dropout))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPILE MODEL\n",
    "\n",
    "**Optimizer:** Adam - Adaptive learning rate\n",
    "\n",
    "**Loss Function:** Categorical Crossentropy - For multi-class classification\n",
    "\n",
    "**Metrics:** Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling Model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Model compiled successfully!\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss Function: Categorical Crossentropy\")\n",
    "print(f\"  Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SETUP CALLBACKS\n",
    "\n",
    "**EarlyStopping:** Stop training if validation loss doesn't improve\n",
    "\n",
    "**ReduceLROnPlateau:** Reduce learning rate when validation loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  âœ“ Early Stopping (patience=5)\")\n",
    "print(\"  âœ“ Learning Rate Reduction (patience=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training CNN Model...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ Training Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TRAINING HISTORY VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', \n",
    "             linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy',\n",
    "             linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss',\n",
    "             linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss',\n",
    "             linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training History Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"\\nBest Validation Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"Best Validation Loss: {min(history.history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. EVALUATE ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Model on Test Set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "if test_accuracy > 0.99:\n",
    "    print(\"\\nðŸ† Excellent! Model achieves >99% accuracy!\")\n",
    "elif test_accuracy > 0.95:\n",
    "    print(\"\\nâœ“ Very Good! Model achieves >95% accuracy!\")\n",
    "elif test_accuracy > 0.90:\n",
    "    print(\"\\nâœ“ Good! Model achieves >90% accuracy!\")\n",
    "else:\n",
    "    print(\"\\nâš  Model could be improved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PREDICTIONS AND ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making Predictions...\")\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "correct = np.sum(y_pred == y_test)\n",
    "total = len(y_test)\n",
    "print(f\"Correct Predictions: {correct}/{total}\")\n",
    "print(f\"Incorrect Predictions: {total - correct}/{total}\")\n",
    "print(f\"Accuracy: {correct/total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=range(10),\n",
    "            yticklabels=range(10),\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            linewidths=0.5, linecolor='white')\n",
    "plt.xlabel('Predicted Digit', fontsize=12)\n",
    "plt.ylabel('True Digit', fontsize=12)\n",
    "plt.title('Confusion Matrix - CNN Digit Classification', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(10):\n",
    "    class_total = np.sum(y_test == i)\n",
    "    class_correct = cm[i, i]\n",
    "    class_accuracy = class_correct / class_total * 100 if class_total > 0 else 0\n",
    "    print(f\"Digit {i}: {class_correct:4d}/{class_total:4d} = {class_accuracy:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. CLASSIFICATION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=[f'Digit {i}' for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. PREDICTION CONFIDENCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "correct_mask = (y_pred == y_test)\n",
    "correct_confidences = confidences[correct_mask]\n",
    "incorrect_confidences = confidences[~correct_mask]\n",
    "\n",
    "print(\"Confidence Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCorrect Predictions:\")\n",
    "print(f\"  Mean Confidence: {correct_confidences.mean()*100:.2f}%\")\n",
    "print(f\"  Median Confidence: {np.median(correct_confidences)*100:.2f}%\")\n",
    "print(f\"  Min Confidence: {correct_confidences.min()*100:.2f}%\")\n",
    "\n",
    "if len(incorrect_confidences) > 0:\n",
    "    print(f\"\\nIncorrect Predictions:\")\n",
    "    print(f\"  Mean Confidence: {incorrect_confidences.mean()*100:.2f}%\")\n",
    "    print(f\"  Median Confidence: {np.median(incorrect_confidences)*100:.2f}%\")\n",
    "    print(f\"  Max Confidence: {incorrect_confidences.max()*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(correct_confidences, bins=50, color='#4ECDC4', alpha=0.7, label='Correct')\n",
    "if len(incorrect_confidences) > 0:\n",
    "    plt.hist(incorrect_confidences, bins=20, color='#FF6B6B', alpha=0.7, label='Incorrect')\n",
    "plt.xlabel('Prediction Confidence', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "confidence_bins = [0.0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0]\n",
    "bin_counts = np.histogram(confidences, bins=confidence_bins)[0]\n",
    "bin_labels = ['0-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-95%', '95-99%', '99-100%']\n",
    "plt.bar(range(len(bin_counts)), bin_counts, color='#45B7D1')\n",
    "plt.xlabel('Confidence Range', fontsize=12)\n",
    "plt.ylabel('Number of Predictions', fontsize=12)\n",
    "plt.title('Predictions by Confidence Level', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(len(bin_labels)), bin_labels, rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. VISUALIZE CORRECT PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices = np.where(y_pred == y_test)[0]\n",
    "sample_correct = np.random.choice(correct_indices, 15, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.suptitle('Correct Predictions with Confidence Scores', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_correct):\n",
    "    ax = axes[idx // 5, idx % 5]\n",
    "    ax.imshow(X_test_original[sample_idx], cmap='gray')\n",
    "    pred_digit = y_pred[sample_idx]\n",
    "    true_digit = y_test[sample_idx]\n",
    "    confidence = confidences[sample_idx] * 100\n",
    "    \n",
    "    ax.set_title(f'True: {true_digit} | Pred: {pred_digit}\\nConf: {confidence:.1f}%',\n",
    "                fontsize=10, color='green', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. VISUALIZE INCORRECT PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_indices = np.where(y_pred != y_test)[0]\n",
    "\n",
    "if len(incorrect_indices) > 0:\n",
    "    num_display = min(15, len(incorrect_indices))\n",
    "    sample_incorrect = incorrect_indices[:num_display]\n",
    "    \n",
    "    rows = (num_display + 4) // 5\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(15, rows*3))\n",
    "    fig.suptitle('Incorrect Predictions - Analysis', fontsize=16, fontweight='bold', y=1.01)\n",
    "    \n",
    "    axes = axes.flatten() if num_display > 5 else [axes] if num_display == 1 else axes\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_incorrect):\n",
    "        if idx < len(axes):\n",
    "            ax = axes[idx]\n",
    "            ax.imshow(X_test_original[sample_idx], cmap='gray')\n",
    "            pred_digit = y_pred[sample_idx]\n",
    "            true_digit = y_test[sample_idx]\n",
    "            confidence = confidences[sample_idx] * 100\n",
    "            \n",
    "            ax.set_title(f'True: {true_digit} | Pred: {pred_digit}\\nConf: {confidence:.1f}%',\n",
    "                        fontsize=10, color='red', fontweight='bold')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    for idx in range(num_display, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nShowing {num_display} incorrect predictions out of {len(incorrect_indices)} total errors\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ Perfect! No incorrect predictions on test set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. PREDICTION EXAMPLES WITH TOP-3 PROBABILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.choice(len(X_test), 6, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Predictions with Probability Distribution', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Create subplot for image\n",
    "    ax_img = plt.subplot(2, 6, row*6 + col*2 + 1)\n",
    "    ax_img.imshow(X_test_original[sample_idx], cmap='gray')\n",
    "    ax_img.set_title(f'True: {y_test[sample_idx]}', fontsize=11, fontweight='bold')\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Create subplot for probabilities\n",
    "    ax_prob = plt.subplot(2, 6, row*6 + col*2 + 2)\n",
    "    probs = y_pred_proba[sample_idx]\n",
    "    top_3_idx = np.argsort(probs)[-3:][::-1]\n",
    "    top_3_probs = probs[top_3_idx]\n",
    "    \n",
    "    colors = ['#4ECDC4' if i == y_test[sample_idx] else '#FF6B6B' \n",
    "              for i in top_3_idx]\n",
    "    \n",
    "    ax_prob.barh(range(3), top_3_probs, color=colors)\n",
    "    ax_prob.set_yticks(range(3))\n",
    "    ax_prob.set_yticklabels([f'{i}' for i in top_3_idx])\n",
    "    ax_prob.set_xlabel('Probability', fontsize=9)\n",
    "    ax_prob.set_title(f'Pred: {y_pred[sample_idx]}', fontsize=11, fontweight='bold')\n",
    "    ax_prob.set_xlim([0, 1])\n",
    "    ax_prob.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (digit, prob) in enumerate(zip(top_3_idx, top_3_probs)):\n",
    "        ax_prob.text(prob + 0.02, i, f'{prob*100:.1f}%', \n",
    "                    va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. MODEL PERFORMANCE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CNN MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset: MNIST Handwritten Digits\")\n",
    "print(f\"Total Training Samples: {len(X_train):,}\")\n",
    "print(f\"Total Test Samples: {len(X_test):,}\")\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  - Convolutional Layers: {sum(1 for layer in model.layers if isinstance(layer, Conv2D))}\")\n",
    "print(f\"  - Pooling Layers: {sum(1 for layer in model.layers if isinstance(layer, MaxPooling2D))}\")\n",
    "print(f\"  - Dense Layers: {sum(1 for layer in model.layers if isinstance(layer, Dense))}\")\n",
    "print(f\"  - Total Parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  - Epochs: {len(history.history['loss'])}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Optimizer: Adam\")\n",
    "print(f\"  - Loss Function: Categorical Crossentropy\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  - Correct Predictions: {correct:,}/{total:,}\")\n",
    "print(f\"  - Incorrect Predictions: {total-correct:,}/{total:,}\")\n",
    "print(f\"  - Average Confidence (Correct): {correct_confidences.mean()*100:.2f}%\")\n",
    "\n",
    "best_digit = np.argmax([cm[i,i]/np.sum(y_test == i) for i in range(10)])\n",
    "worst_digit = np.argmin([cm[i,i]/np.sum(y_test == i) for i in range(10)])\n",
    "\n",
    "print(f\"\\nPer-Digit Performance:\")\n",
    "print(f\"  - Best Classified Digit: {best_digit}\")\n",
    "print(f\"  - Worst Classified Digit: {worst_digit}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ MODEL EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving Model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.save('cnn_mnist_model.h5')\n",
    "print(\"âœ“ Model saved as 'cnn_mnist_model.h5'\")\n",
    "\n",
    "model.save('cnn_mnist_model')\n",
    "print(\"âœ“ Model saved in SavedModel format as 'cnn_mnist_model/'\")\n",
    "\n",
    "print(\"\\nModel can be loaded using:\")\n",
    "print(\"  model = keras.models.load_model('cnn_mnist_model.h5')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. KEY FINDINGS & CONCLUSIONS\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "Our CNN successfully combines:\n",
    "- **Convolutional Layers**: Extract spatial features from images\n",
    "- **Batch Normalization**: Stabilize and speed up training\n",
    "- **MaxPooling**: Reduce dimensionality and computation\n",
    "- **Dropout**: Prevent overfitting\n",
    "- **Dense Layers**: Final classification\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "- **Optimizer**: Adam - Adaptive learning rate for efficient training\n",
    "- **Callbacks**: Early stopping and learning rate reduction for optimal performance\n",
    "- **Validation Split**: Monitor overfitting during training\n",
    "\n",
    "### Performance:\n",
    "\n",
    "The model achieves excellent accuracy on MNIST:\n",
    "- High test accuracy (>99% typical)\n",
    "- Low loss values\n",
    "- Balanced performance across all digit classes\n",
    "- High prediction confidence for correct classifications\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Data Preprocessing Matters**: Normalization improves convergence\n",
    "2. **Architecture Depth**: Multiple conv layers extract hierarchical features\n",
    "3. **Regularization**: Dropout and BatchNorm prevent overfitting\n",
    "4. **Confidence Analysis**: Model is highly confident in correct predictions\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "1. **Data Augmentation**: Rotation, shifting, zooming for robustness\n",
    "2. **Deeper Architecture**: More layers for complex patterns\n",
    "3. **Advanced Techniques**: Residual connections, attention mechanisms\n",
    "4. **Ensemble Methods**: Combine multiple models\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- **OCR Systems**: Document digitization\n",
    "- **Bank Check Processing**: Automated check reading\n",
    "- **Form Recognition**: Automated form processing\n",
    "- **Postal Services**: ZIP code recognition\n",
    "\n",
    "---\n",
    "\n",
    "## CONCLUSION\n",
    "\n",
    "This CNN model demonstrates the power of deep learning for image classification:\n",
    "\n",
    "âœ… **High Accuracy**: Excellent performance on test data\n",
    "\n",
    "âœ… **Robust Architecture**: Well-designed with proper regularization\n",
    "\n",
    "âœ… **Fast Training**: Efficient convergence with callbacks\n",
    "\n",
    "âœ… **Production Ready**: Saved model ready for deployment\n",
    "\n",
    "The model successfully learns to classify handwritten digits with minimal errors, showcasing the effectiveness of CNNs for computer vision tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**CODTECH Internship Task Completed Successfully! âœ“**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
